{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, param: dict):\n",
    "        super().__init__()\n",
    "        ci = 1  # input chanel size\n",
    "        kernel_num = param['kernel_num'] # output chanel size\n",
    "        kernel_size = param['kernel_size']\n",
    "        vocab_size = param['vocab_size']\n",
    "        embed_dim = param['embed_dim']\n",
    "        dropout = param['dropout']\n",
    "        class_num = param['class_num']\n",
    "        self.param = param\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=1)\n",
    "        self.conv11 = nn.Conv2d(ci, kernel_num, (kernel_size[0], embed_dim))\n",
    "        self.conv12 = nn.Conv2d(ci, kernel_num, (kernel_size[1], embed_dim))\n",
    "        self.conv13 = nn.Conv2d(ci, kernel_num, (kernel_size[2], embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(len(kernel_size) * kernel_num, class_num)\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_and_pool(x, conv):\n",
    "        # x: (batch, 1, sentence_length, embed_dim)\n",
    "        x = conv(x)\n",
    "        # x: (batch, kernel_num, H_out, 1)\n",
    "        x = F.relu(x.squeeze(3))\n",
    "        # x: (batch, kernel_num, H_out)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        #  (batch, kernel_num)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, sentence_length)\n",
    "        x = self.embed(x)\n",
    "        # x: (batch, sentence_length, embed_dim)\n",
    "        # TODO init embed matrix with pre-trained\n",
    "        x = x.unsqueeze(1)\n",
    "        # x: (batch, 1, sentence_length, embed_dim)\n",
    "        x1 = self.conv_and_pool(x, self.conv11)  # (batch, kernel_num)\n",
    "        x2 = self.conv_and_pool(x, self.conv12)  # (batch, kernel_num)\n",
    "        x3 = self.conv_and_pool(x, self.conv13)  # (batch, kernel_num)\n",
    "        x = torch.cat((x1, x2, x3), 1)  # (batch, 3 * kernel_num)\n",
    "        x = self.dropout(x)\n",
    "        logit = F.log_softmax(self.fc1(x), dim=1)\n",
    "        # logit = F.softmax(self.fc1(x), dim=1)\n",
    "        # logit = self.fc1(x)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    s = []\n",
    "    label = []\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    for i in data:\n",
    "        label.append(i.split(\" \")[0])\n",
    "        s.append(i.strip(\"\\n\").split(\" \")[1:])\n",
    "    return s, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y = read_data('/Users/zhouzhirui/project/Task-Oriented-Chatbot/corpus/intent/fastText/demo.train.txt')\n",
    "test_x, test_y = read_data('/Users/zhouzhirui/project/Task-Oriented-Chatbot/corpus/intent/fastText/demo.test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2id = {\"UNK\": 0, \"PAD\": 1}\n",
    "idx = 2\n",
    "for s in train_x:\n",
    "    for w in s:\n",
    "        if w in word2id:\n",
    "            pass\n",
    "        else:\n",
    "            word2id[w] = idx\n",
    "            idx += 1\n",
    "\n",
    "label2id = {}\n",
    "label_idx = 0\n",
    "for l in train_y:\n",
    "    if l in label2id:\n",
    "        pass\n",
    "    else:\n",
    "        label2id[l] = label_idx\n",
    "        label_idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text(sentences, d, max_length=10):\n",
    "    result = []\n",
    "    for s in sentences:\n",
    "        s = s[:max_length]\n",
    "        s = [\"PAD\"] * (max_length - len(s)) + s\n",
    "        s = [d.get(w, 0) for w in s]\n",
    "        result.append(s)\n",
    "    return np.array(result)\n",
    "\n",
    "def conver_label(labels, d):\n",
    "    result = [d[l] for l in labels]\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = convert_text(train_x, word2id)\n",
    "train_y = conver_label(train_y, label2id)\n",
    "test_x = convert_text(test_x, word2id)\n",
    "test_y = conver_label(test_y, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch=100):\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    size = x.shape[0]\n",
    "    idx = np.array(list(range(0, size)))\n",
    "    np.random.shuffle(idx)\n",
    "    x = x[idx].copy()\n",
    "    y = y[idx].copy()\n",
    "    n = size // batch\n",
    "    for i in range(n):\n",
    "        yield x[batch*i: batch*(i+1)], y[batch*i: batch*(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.tensor(test_x)\n",
    "test_y = torch.tensor(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, lr, epochs):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#     optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        for step, (x, y) in enumerate(get_batch(train_x, train_y)):\n",
    "            x = torch.tensor(x)\n",
    "            y = torch.tensor(y)\n",
    "            optimizer.zero_grad()\n",
    "            logit = model(x)\n",
    "            loss = F.cross_entropy(logit, y)\n",
    "            # loss = F.nll_loss(logit, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if step % 100 == 0:\n",
    "                model.eval()\n",
    "                eval_logit = model(test_x)\n",
    "                eval_loss = F.cross_entropy(eval_logit, test_y)\n",
    "                model.train()\n",
    "                print(\"epoch: {:>2}, step: {:>4} ,train loss: {:.6f}, eval loss: {:.6f}\".format(epoch, step, loss, eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textCNN_param = {\n",
    "    \"vocab_size\": len(word2id),\n",
    "    \"embed_dim\": 40,\n",
    "    \"class_num\": len(label2id),\n",
    "    \"kernel_num\": 16,\n",
    "    \"kernel_size\": [3, 4, 5],\n",
    "    \"dropout\": 0.5,\n",
    "}\n",
    "\n",
    "model = TextCNN(textCNN_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, step:    0 ,train loss: 4.477167, eval loss: 4.021335\n",
      "epoch:  0, step:  100 ,train loss: 1.443769, eval loss: 0.983097\n",
      "epoch:  0, step:  200 ,train loss: 0.954692, eval loss: 0.652551\n",
      "epoch:  0, step:  300 ,train loss: 0.638155, eval loss: 0.524966\n",
      "epoch:  1, step:    0 ,train loss: 0.514146, eval loss: 0.460144\n",
      "epoch:  1, step:  100 ,train loss: 0.721711, eval loss: 0.420159\n",
      "epoch:  1, step:  200 ,train loss: 0.815014, eval loss: 0.392086\n",
      "epoch:  1, step:  300 ,train loss: 0.485019, eval loss: 0.383653\n",
      "epoch:  2, step:    0 ,train loss: 0.676553, eval loss: 0.354957\n",
      "epoch:  2, step:  100 ,train loss: 0.292083, eval loss: 0.349564\n",
      "epoch:  2, step:  200 ,train loss: 0.656120, eval loss: 0.352064\n",
      "epoch:  2, step:  300 ,train loss: 0.283528, eval loss: 0.335862\n",
      "epoch:  3, step:    0 ,train loss: 0.396631, eval loss: 0.334483\n",
      "epoch:  3, step:  100 ,train loss: 0.588300, eval loss: 0.329411\n",
      "epoch:  3, step:  200 ,train loss: 0.537329, eval loss: 0.336878\n",
      "epoch:  3, step:  300 ,train loss: 0.406257, eval loss: 0.324261\n",
      "epoch:  0, step:    0 ,train loss: 0.340085, eval loss: 0.330258\n",
      "epoch:  0, step:  100 ,train loss: 0.303160, eval loss: 0.328051\n",
      "epoch:  0, step:  200 ,train loss: 0.439840, eval loss: 0.322677\n",
      "epoch:  0, step:  300 ,train loss: 0.380926, eval loss: 0.321184\n",
      "epoch:  1, step:    0 ,train loss: 0.318906, eval loss: 0.317942\n",
      "epoch:  1, step:  100 ,train loss: 0.175646, eval loss: 0.313216\n",
      "epoch:  1, step:  200 ,train loss: 0.265754, eval loss: 0.315335\n",
      "epoch:  1, step:  300 ,train loss: 0.226718, eval loss: 0.317030\n",
      "epoch:  2, step:    0 ,train loss: 0.331034, eval loss: 0.310386\n",
      "epoch:  2, step:  100 ,train loss: 0.179175, eval loss: 0.314418\n",
      "epoch:  2, step:  200 ,train loss: 0.227620, eval loss: 0.314781\n",
      "epoch:  2, step:  300 ,train loss: 0.348102, eval loss: 0.317407\n",
      "epoch:  3, step:    0 ,train loss: 0.320333, eval loss: 0.314315\n",
      "epoch:  3, step:  100 ,train loss: 0.268350, eval loss: 0.316036\n",
      "epoch:  3, step:  200 ,train loss: 0.216575, eval loss: 0.313698\n",
      "epoch:  3, step:  300 ,train loss: 0.296362, eval loss: 0.316229\n",
      "epoch:  4, step:    0 ,train loss: 0.340766, eval loss: 0.318203\n",
      "epoch:  4, step:  100 ,train loss: 0.206654, eval loss: 0.313756\n",
      "epoch:  4, step:  200 ,train loss: 0.354482, eval loss: 0.317120\n",
      "epoch:  4, step:  300 ,train loss: 0.057348, eval loss: 0.315635\n",
      "epoch:  5, step:    0 ,train loss: 0.197554, eval loss: 0.316255\n",
      "epoch:  5, step:  100 ,train loss: 0.143690, eval loss: 0.314807\n",
      "epoch:  5, step:  200 ,train loss: 0.137143, eval loss: 0.323610\n",
      "epoch:  5, step:  300 ,train loss: 0.147823, eval loss: 0.319026\n"
     ]
    }
   ],
   "source": [
    "train(model, 0.01, 4)\n",
    "train(model, 0.001, 6)\n",
    "# train(model, 0.0001, 4)\n",
    "# train(model, 0.00001, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9419343131917982\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(test_x)\n",
    "c = 0\n",
    "for idx, i in enumerate(torch.argmax(y_pred, 1)):\n",
    "    if test_y[idx] == i:\n",
    "        c += 1\n",
    "print(c / len(test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
